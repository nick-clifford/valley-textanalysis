{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Clifford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use mallet to help produce topic models for my corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/nickclifford/Documents/UVA/Spring 2020/DS 5001 Exploratory Text Analysis/final/data/'\n",
    "table_out = datadir + 'tables/'\n",
    "\n",
    "LIB = pd.read_csv(datadir + 'tables/LIB_mod.csv', parse_dates=['date']).set_index('doc_id')\n",
    "TOKEN = pd.read_csv(datadir + 'tables/TOKEN_mod.csv').set_index('doc_id')\n",
    "VOCAB = pd.read_csv(datadir + 'tables/VOCAB_mod.csv').set_index('term_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 4000\n",
    "n_topics = 30\n",
    "max_iter = 5\n",
    "OHCO = ['doc_id', 'page_num', 'para_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I make sure to format the corpus correctly before using mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>A7186</td>\n",
       "      <td>[p, s, since, writing, the, foregoing, mr, tuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>F3067</td>\n",
       "      <td>[my, dear, husband, i, take, my, pen, in, hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>A7179</td>\n",
       "      <td>[dear, sir, yours, of, the, 23rd, is, received...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>F0920</td>\n",
       "      <td>[chambersburg, dear, sir, i, have, just, bid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>F3044</td>\n",
       "      <td>[morris, iland, south, carlina, my, dear, i, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>A8018</td>\n",
       "      <td>[richd, dear, brother, william, h, clarke, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>A5041</td>\n",
       "      <td>[fairfax, co, va, dear, sister, i, take, my, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>F3010</td>\n",
       "      <td>[jacksonville, florida, headquarters, my, dear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>F3503</td>\n",
       "      <td>[london, franklin, co, pa, i, do, hereby, cert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>A7154</td>\n",
       "      <td>[yours, containing, checks, one, for, 627100, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                           doc_text\n",
       "935   A7186  [p, s, since, writing, the, foregoing, mr, tuk...\n",
       "1604  F3067  [my, dear, husband, i, take, my, pen, in, hand...\n",
       "928   A7179  [dear, sir, yours, of, the, 23rd, is, received...\n",
       "1475  F0920  [chambersburg, dear, sir, i, have, just, bid, ...\n",
       "1581  F3044  [morris, iland, south, carlina, my, dear, i, t...\n",
       "1088  A8018  [richd, dear, brother, william, h, clarke, of,...\n",
       "563   A5041  [fairfax, co, va, dear, sister, i, take, my, p...\n",
       "1547  F3010  [jacksonville, florida, headquarters, my, dear...\n",
       "1611  F3503  [london, franklin, co, pa, i, do, hereby, cert...\n",
       "903   A7154  [yours, containing, checks, one, for, 627100, ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = TOKEN\\\n",
    ".groupby(BAG)\\\n",
    ".term_str.apply(lambda  x:  x.tolist())\\\n",
    ".reset_index().rename({'term_str':'doc_text'}, axis=1)\n",
    "corpus.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corpus is moved to polite/corpus directory after read to .csv\n",
    "#corpus.to_csv(datadir + '/tables/valley-corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Mallet ouput files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/valley-mazo-output/tables/DOC.csv',\n",
       " 'data/valley-mazo-output/tables/DOCTOPIC.csv',\n",
       " 'data/valley-mazo-output/tables/DOCTOPIC_NARROW.csv',\n",
       " 'data/valley-mazo-output/tables/DOCWORD.csv',\n",
       " 'data/valley-mazo-output/tables/TOPIC.csv',\n",
       " 'data/valley-mazo-output/tables/TOPICPHRASE.csv',\n",
       " 'data/valley-mazo-output/tables/TOPICWORD.csv',\n",
       " 'data/valley-mazo-output/tables/TOPICWORD_DIAGS.csv',\n",
       " 'data/valley-mazo-output/tables/TOPICWORD_NARROW.csv',\n",
       " 'data/valley-mazo-output/tables/TOPICWORD_WEIGHTS.csv',\n",
       " 'data/valley-mazo-output/tables/VOCAB.csv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = !ls data/valley-mazo-output/tables/*.csv\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for table in tables:\n",
    "    table_name = table.split('/')[-1].split('.')[0]\n",
    "    df[table_name] = pd.read_csv(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOC':       doc_id  src_doc_id doc_label\n",
       " 0          0         NaN    doc_id\n",
       " 1          1         0.0     A0001\n",
       " 2          2         1.0     A0002\n",
       " 3          3         2.0     A0003\n",
       " 4          4         3.0     A0004\n",
       " ...      ...         ...       ...\n",
       " 1838    1838      1837.0     F8578\n",
       " 1839    1839      1838.0     F8579\n",
       " 1840    1840      1839.0     F8580\n",
       " 1841    1841      1840.0     F8581\n",
       " 1842    1842      1841.0     F8582\n",
       " \n",
       " [1843 rows x 3 columns],\n",
       " 'DOCTOPIC':       doc_id         0         1         2         3         4         5  \\\n",
       " 0          0  0.081774  0.266344  0.063659  0.047258  0.102270  0.265975   \n",
       " 1          1  0.107574  0.034277  0.129395  0.028252  0.001757  0.051321   \n",
       " 2          2  0.005033  0.428743  0.003918  0.258849  0.006295  0.002152   \n",
       " 3          3  0.007325  0.272156  0.295384  0.086999  0.154001  0.106590   \n",
       " 4          4  0.033574  0.747677  0.006833  0.054665  0.010977  0.028549   \n",
       " ...      ...       ...       ...       ...       ...       ...       ...   \n",
       " 1838    1838  0.002008  0.006539  0.035590  0.103242  0.070565  0.772145   \n",
       " 1839    1839  0.088425  0.025584  0.014018  0.001260  0.002727  0.764814   \n",
       " 1840    1840  0.050297  0.096201  0.002181  0.017451  0.003504  0.705709   \n",
       " 1841    1841  0.073463  0.001201  0.112116  0.001607  0.066306  0.731581   \n",
       " 1842    1842  0.020590  0.010085  0.028650  0.019283  0.082593  0.823514   \n",
       " \n",
       "              6         7         8         9  \n",
       " 0     0.023522  0.056294  0.060208  0.032695  \n",
       " 1     0.087787  0.137879  0.417933  0.003825  \n",
       " 2     0.001448  0.174092  0.117457  0.002012  \n",
       " 3     0.002107  0.046425  0.026084  0.002929  \n",
       " 4     0.002525  0.006043  0.105647  0.003509  \n",
       " ...        ...       ...       ...       ...  \n",
       " 1838  0.000577  0.007053  0.001478  0.000803  \n",
       " 1839  0.000627  0.001501  0.001606  0.099437  \n",
       " 1840  0.000806  0.112751  0.002063  0.009036  \n",
       " 1841  0.000800  0.001914  0.009901  0.001112  \n",
       " 1842  0.000891  0.002131  0.011026  0.001238  \n",
       " \n",
       " [1843 rows x 11 columns],\n",
       " 'DOCTOPIC_NARROW':        doc_id  topic_id  topic_weight  topic_weight_zscore\n",
       " 0           0         0      0.081774            -0.100295\n",
       " 1           1         0      0.107574             0.041681\n",
       " 2           2         0      0.005033            -0.522587\n",
       " 3           3         0      0.007325            -0.509979\n",
       " 4           4         0      0.033574            -0.365534\n",
       " ...       ...       ...           ...                  ...\n",
       " 18425    1838         9      0.000803            -0.545868\n",
       " 18426    1839         9      0.099437            -0.003097\n",
       " 18427    1840         9      0.009036            -0.500560\n",
       " 18428    1841         9      0.001112            -0.544168\n",
       " 18429    1842         9      0.001238            -0.543473\n",
       " \n",
       " [18430 rows x 4 columns],\n",
       " 'DOCWORD':         doc_id  word_id  word_pos  topic_id\n",
       " 0            1        0         0         6\n",
       " 1            1        1         1         5\n",
       " 2            1        2         2         8\n",
       " 3            1        3         3         8\n",
       " 4            1        4         4         7\n",
       " ...        ...      ...       ...       ...\n",
       " 277479    1842      185       107         5\n",
       " 277480    1842      152       108         5\n",
       " 277481    1842    20906       109         5\n",
       " 277482    1842    11853       110         5\n",
       " 277483    1842     5137       111         5\n",
       " \n",
       " [277484 rows x 4 columns],\n",
       " 'TOPIC':    topic_id  topic_alpha                                        topic_words  \\\n",
       " 0         0      0.35399  write home time letter week cousin hear love d...   \n",
       " 1         1      0.15296  regiment company colonel general virginia capt...   \n",
       " 2         2      0.27557  dear time friend hope god letter long good fee...   \n",
       " 3         3      0.20457  money pay business andc family make send john ...   \n",
       " 4         4      0.44271  letter write time home hope good dear present ...   \n",
       " 5         5      0.15136  state man party people union made county men c...   \n",
       " 6         6      0.10182  school people house colored work schools night...   \n",
       " 7         7      0.24369  good send write home gen today andc hope days ...   \n",
       " 8         8      0.26063  men enemy miles back wounded fight river left ...   \n",
       " 9         9      0.14153  love dear heart home happy letter long send ti...   \n",
       " \n",
       "    topic_alpha_zscore      topic_gloss  topic_tokens  topic_document_entropy  \\\n",
       " 0            1.206486       great deal         38712                  6.5571   \n",
       " 1           -0.796205  county virginia         12777                  5.9731   \n",
       " 2            0.425254      dear friend         36453                  6.3471   \n",
       " 3           -0.282058         dear sir         19615                  6.1840   \n",
       " 4            2.090328        glad hear         45646                  6.7472   \n",
       " 5           -0.812144    simon cameron         19690                  5.8851   \n",
       " 6           -1.305669   colored people         13316                  5.3867   \n",
       " 7            0.107661          gen lee         32795                  6.1955   \n",
       " 8            0.276420   killed wounded         39958                  6.1901   \n",
       " 9           -0.910072        give love         18522                  5.6594   \n",
       " \n",
       "    topic_word_length  topic_coherence  topic_uniform_dist  topic_corpus_dist  \\\n",
       " 0                4.4         -46.8786              3.1960             0.8544   \n",
       " 1                6.4         -70.4749              2.9146             1.8825   \n",
       " 2                4.3         -51.1448              2.6058             1.0162   \n",
       " 3                5.1         -65.7132              2.8519             1.3826   \n",
       " 4                4.6         -38.1753              3.5964             0.8101   \n",
       " 5                4.7         -65.0554              2.6086             1.6240   \n",
       " 6                6.1         -47.1624              3.1742             1.7216   \n",
       " 7                4.0         -43.6739              3.1064             0.9891   \n",
       " 8                5.0         -44.8798              3.0899             1.1292   \n",
       " 9                4.8         -37.6820              4.0762             1.3662   \n",
       " \n",
       "    topic_eff_num_words  topic_token_doc_diff  topic_rank_1_docs  \\\n",
       " 0             315.4803                0.0025             0.2000   \n",
       " 1             454.1216                0.0020             0.1065   \n",
       " 2             625.1167                0.0007             0.1398   \n",
       " 3             548.8946                0.0021             0.1615   \n",
       " 4             197.7943                0.0017             0.2599   \n",
       " 5             712.2786                0.0032             0.2084   \n",
       " 6             279.7239                0.0059             0.1752   \n",
       " 7             358.7263                0.0029             0.1552   \n",
       " 8             377.0151                0.0012             0.1631   \n",
       " 9              93.4565                0.0161             0.1382   \n",
       " \n",
       "    topic_allocation_ratio  topic_allocation_count  topic_exclusivity  \n",
       " 0                  0.1081                  0.2265             0.2991  \n",
       " 1                  0.0822                  0.1157             0.5528  \n",
       " 2                  0.0706                  0.1581             0.2279  \n",
       " 3                  0.1367                  0.1837             0.4294  \n",
       " 4                  0.1969                  0.2905             0.3107  \n",
       " 5                  0.1656                  0.2171             0.4829  \n",
       " 6                  0.2042                  0.1801             0.7260  \n",
       " 7                  0.1111                  0.1904             0.3188  \n",
       " 8                  0.1030                  0.2112             0.7120  \n",
       " 9                  0.1588                  0.1432             0.4573  ,\n",
       " 'TOPICPHRASE':     topic_id     topic_phrase  phrase_weight  phrase_count\n",
       " 0          0       great deal       0.002093            13\n",
       " 1          0        give love       0.001610            10\n",
       " 2          0    harpers ferry       0.001449             9\n",
       " 3          0  received letter       0.001288             8\n",
       " 4          0       time write       0.001127             7\n",
       " ..       ...              ...            ...           ...\n",
       " 95         9        send love       0.001553             3\n",
       " 96         9        love kiss       0.001553             3\n",
       " 97         9      sibert dear       0.001553             3\n",
       " 98         9       evans dear       0.001553             3\n",
       " 99         9    richmond dear       0.001553             3\n",
       " \n",
       " [100 rows x 4 columns],\n",
       " 'TOPICWORD':        word_id         0         1         2         3         4         5  \\\n",
       " 0            0  0.000207  0.003835  0.000384  0.000051  0.000000  0.000000   \n",
       " 1            1  0.000000  0.000000  0.000000  0.002498  0.000000  0.001422   \n",
       " 2            2  0.000000  0.000157  0.000000  0.000000  0.000372  0.000000   \n",
       " 3            3  0.000000  0.004618  0.000247  0.000000  0.001095  0.000000   \n",
       " 4            4  0.000594  0.000000  0.000000  0.000051  0.000394  0.002946   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 22746    22746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000051   \n",
       " 22747    22747  0.000000  0.000000  0.000000  0.000000  0.000000  0.000051   \n",
       " 22748    22748  0.000000  0.000000  0.000000  0.000000  0.000000  0.000051   \n",
       " 22749    22749  0.000000  0.000000  0.000000  0.000000  0.000000  0.000051   \n",
       " 22750    22750  0.000000  0.000000  0.000000  0.000000  0.000000  0.000051   \n",
       " \n",
       "               6         7         8         9  \n",
       " 0      0.000751  0.000000  0.000000  0.000000  \n",
       " 1      0.000000  0.001189  0.001126  0.000216  \n",
       " 2      0.000000  0.001891  0.002202  0.000000  \n",
       " 3      0.000000  0.004696  0.007558  0.000000  \n",
       " 4      0.000901  0.000732  0.001827  0.000000  \n",
       " ...         ...       ...       ...       ...  \n",
       " 22746  0.000000  0.000000  0.000000  0.000000  \n",
       " 22747  0.000000  0.000000  0.000000  0.000000  \n",
       " 22748  0.000000  0.000000  0.000000  0.000000  \n",
       " 22749  0.000000  0.000000  0.000000  0.000000  \n",
       " 22750  0.000000  0.000000  0.000000  0.000000  \n",
       " \n",
       " [22751 rows x 11 columns],\n",
       " 'TOPICWORD_DIAGS':     topic_id  word_id  word_str  rank  count     prob  cumulative  docs  \\\n",
       " 0          0      762     write     1    680  0.01757     0.01757   432   \n",
       " 1          0       23      home     2    680  0.01757     0.03513   380   \n",
       " 2          0       31      time     3    567  0.01465     0.04978   381   \n",
       " 3          0       14    letter     4    542  0.01400     0.06378   346   \n",
       " 4          0      744      week     5    419  0.01082     0.07460   274   \n",
       " ..       ...      ...       ...   ...    ...      ...         ...   ...   \n",
       " 95         9       14    letter     6    271  0.01463     0.17396   155   \n",
       " 96         9       13      long     7    219  0.01182     0.18578   135   \n",
       " 97         9      547      send     8    217  0.01172     0.19749   130   \n",
       " 98         9       31      time     9    212  0.01145     0.20894   141   \n",
       " 99         9      404  saturday    10    210  0.01134     0.22028   106   \n",
       " \n",
       "     word_length  coherence  uniform_dist  corpus_dist  token_doc_diff  \\\n",
       " 0             5     0.0000        0.1052       0.0109          0.0000   \n",
       " 1             4    -0.8056        0.1052       0.0135          0.0005   \n",
       " 2             4    -0.7925        0.0851       0.0054          0.0000   \n",
       " 3             6    -0.8220        0.0807       0.0067          0.0000   \n",
       " 4             4    -1.0777        0.0596       0.0147          0.0000   \n",
       " ..          ...        ...           ...          ...             ...   \n",
       " 95            6    -1.0165        0.0850       0.0076          0.0010   \n",
       " 96            4    -1.0664        0.0662       0.0120          0.0013   \n",
       " 97            4    -1.1012        0.0654       0.0082          0.0011   \n",
       " 98            4    -1.1651        0.0637       0.0014          0.0018   \n",
       " 99            8    -1.3511        0.0630       0.0224          0.0003   \n",
       " \n",
       "     exclusivity  \n",
       " 0        0.2435  \n",
       " 1        0.2596  \n",
       " 2        0.1785  \n",
       " 3        0.2076  \n",
       " 4        0.4234  \n",
       " ..          ...  \n",
       " 95       0.2093  \n",
       " 96       0.3097  \n",
       " 97       0.2048  \n",
       " 98       0.1346  \n",
       " 99       0.6535  \n",
       " \n",
       " [100 rows x 14 columns],\n",
       " 'TOPICWORD_NARROW':        word_id  topic_id  word_count\n",
       " 0            0         1          49\n",
       " 1            0         2          14\n",
       " 2            0         6          10\n",
       " 3            0         0           8\n",
       " 4            0         3           1\n",
       " ...        ...       ...         ...\n",
       " 37782    22746         5           1\n",
       " 37783    22747         5           1\n",
       " 37784    22748         5           1\n",
       " 37785    22749         5           1\n",
       " 37786    22750         5           1\n",
       " \n",
       " [37787 rows x 3 columns],\n",
       " 'TOPICWORD_WEIGHTS':         topic_id       word_str   word_wgt\n",
       " 0              0       military   8.059133\n",
       " 1              0          court   0.059133\n",
       " 2              0          corps   0.059133\n",
       " 3              0           army   0.059133\n",
       " 4              0          north  23.059133\n",
       " ...          ...            ...        ...\n",
       " 227505         9  stereotypical   0.059133\n",
       " 227506         9  halfbutchered   0.059133\n",
       " 227507         9  embarrasments   0.059133\n",
       " 227508         9   reconcilable   0.059133\n",
       " 227509         9   censuredmost   0.059133\n",
       " \n",
       " [227510 rows x 3 columns],\n",
       " 'VOCAB':        word_id       word_str\n",
       " 0            0       military\n",
       " 1            1          court\n",
       " 2            2          corps\n",
       " 3            3           army\n",
       " 4            4          north\n",
       " ...        ...            ...\n",
       " 22746    22746  stereotypical\n",
       " 22747    22747  halfbutchered\n",
       " 22748    22748  embarrasments\n",
       " 22749    22749   reconcilable\n",
       " 22750    22750   censuredmost\n",
       " \n",
       " [22751 rows x 2 columns]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(tree, labels):\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(figsize=(5, 10))\n",
    "    dendrogram = sch.dendrogram(tree, labels=labels, orientation=\"left\")\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS = pdist(normalize(PHI), metric='euclidean')\n",
    "TREE = sch.linkage(SIMS, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels  = [\"{}: {}\".format(a,b) for a, b in zip(AUTHORS.index,  AUTHORS.topterms.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(TREE, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Space\n",
    "\n",
    "We use Scikit Learn's CountVectorizer to convert our F1 corpus of paragraphs into a document-term vector space of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = CountVectorizer(max_features=n_terms, stop_words='english')\n",
    "tf = tfv.fit_transform(corpus)\n",
    "TERMS = tfv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Generate Model\n",
    "\n",
    "We run Scikit Learn's [LatentDirichletAllocation algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation) and extract the THETA and PHI tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=n_topics, max_iter=max_iter, learning_offset=50., random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
