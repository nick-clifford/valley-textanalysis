{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Clifford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper for Valley of the Shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import requests \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the pages that host the various documents are listed on a `.../papers/` directory of the Valley of the Shadow website. In order to crawl the website, I must first gather the complete list of document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2746 total document IDs:\n",
      "\n",
      "[ A0001.html, A0002.html, ... FN090664.html ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://valley.lib.virginia.edu/papers/\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html')\n",
    "# document ids that correspond to hyperlink names\n",
    "doc_ids = soup.get_text().split('\\n')[1:-5]\n",
    "\n",
    "print(\"%d total document IDs:\\n\\n[ %s, %s, ... %s ]\\n\" %(len(doc_ids), doc_ids[0], doc_ids[1], doc_ids[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A#### = Augusta County letters \n",
    "- F#### = Franklin County letters\n",
    "- B#### = Freedmen's Bureau records\n",
    "- @D#### = Diary entries\n",
    "- @N#### = Newspaper Editorial\n",
    "- Br@@@, Em@@ = Other? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I refrain from pulling data off sources that are not letters from Augusta or Franklin County. Here I update my list of feasible IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867 total letter IDs:\n",
      "\n",
      "[ A0001.html, A0002.html, ... F8582.html ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "letter_bool = ~pd.Series(doc_ids).str.startswith(('B','Em','AD','AN','FD','FN'))\n",
    "letter_ids = list(pd.Series(doc_ids)[letter_bool])\n",
    "\n",
    "print(\"%d total letter IDs:\\n\\n[ %s, %s, ... %s ]\\n\" %(len(letter_ids), letter_ids[0], letter_ids[1], letter_ids[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Letters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each document, I scrape the document text in each `<p>` tags, as well as the author/date/county which are located in `<h4>` tags. The result is a combined metadata header and document text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    \n",
    "    # Match all tags, but replace <br/> tag with newline char \\n\n",
    "    clean = re.sub('<(?!br\\/).*?>', '', text)\n",
    "    # Clean out extra whitespace within paragraphs\n",
    "    clean = re.sub('\\s{2,}', ' ', clean)\n",
    "    return re.sub('<br\\/>', '\\n', clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(d_list):\n",
    "    \"\"\"Given a list of strings containing a month, numerical day & year\n",
    "    returns a string in order of Month 00 0000\"\"\"\n",
    "    \n",
    "    # list of possible month strings\n",
    "    month_list = ['january', 'february', 'march', 'april', 'may', 'june',\n",
    " 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    new_d_list = []\n",
    "    for i in d_list:\n",
    "        # remove non word/digit characters\n",
    "        i = re.sub(\"\\W\", \"\", i) \n",
    "        if i in month_list:\n",
    "            # if str is a month, do nothing\n",
    "            new_d_list.append(i)\n",
    "        else:\n",
    "            # remove non digit characters\n",
    "            i = re.sub(\"\\D\", \"\", i) \n",
    "            new_d_list.append(i)\n",
    "            \n",
    "    d_str = ' '.join(new_d_list).strip()\n",
    "    # handle datetime conversion for incorrect date\n",
    "    if d_str == 'april 31 1868': \n",
    "        d_str = 'april 30 1868'\n",
    "        \n",
    "    return d_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(url):\n",
    "    \"\"\"Extract the document author(s)/date/text from webpage\"\"\"\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html')\n",
    "    \n",
    "    # Extract metadata\n",
    "    title = re.sub('\\s{2,}', ' ', soup.h4.get_text()).strip()\n",
    "    county, other = title.split(': ')[0], title.split(': ')[1]\n",
    "    if ' to ' in other:\n",
    "        authors = other.split(' to ')[0]\n",
    "        date = ' '.join(other.split(' to ')[1].split(', ')[-2:]).lower()\n",
    "    elif 'Letter from ' in other:   \n",
    "        authors = other.split(', ')[0].lstrip('Letter from ') \n",
    "        date = ' '.join(other.split(', ')[-2:]).lower()\n",
    "        # If the doc is a Will\n",
    "#    elif 'Will of ' in other:   \n",
    "#        authors = other.split(', ')[0].strip('Will of ') \n",
    "#        date = ' '.join(other.split(', ')[1:])\n",
    "    # Remove chars from unclear dates\n",
    "    date = clean_date(date.split())\n",
    "    \n",
    "    # Extract text:\n",
    "    # remove first 2 tags, which are summary sections of the webpage\n",
    "    text_html = soup.find_all(class_='p10')[2:] \n",
    "    text = []\n",
    "    # clean out html and fix whitespace in each paragraph tag\n",
    "    for para_html in text_html: \n",
    "        para = remove_html_tags(str(para_html))\n",
    "        text.append(para)\n",
    "    # further remove extra whitespace from html code and place newlines between text of diff paragraph tags\n",
    "    text = '\\n\\n'.join(text)\n",
    "    text = re.sub('(?<=\\n)[^\\S\\r\\n]', '', text)\n",
    "    # clean out html encoded symbols (&)\n",
    "    text = re.sub('&amp;', '&', text)\n",
    "    \n",
    "    # Combine metadata with text\n",
    "    full_doc = '\\n'.join([authors, county, str(date), '\\n***START OF TEXT***\\n[page 1]\\n', text])\n",
    "    return full_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza H. Stanton\n",
      "Augusta County\n",
      "january 1858\n",
      "\n",
      "***START OF TEXT***\n",
      "[page 1]\n",
      "\n",
      "\n",
      "\n",
      "Staunton Va\n",
      "\n",
      "Good morning Miss Mag\n",
      "\n",
      "I have got some news for you this morning I have not heard from you some time, therefore I cannot tell what are your plans for the future, When you rote me last, you wish me to try to get you a place here, I will not say that I have got you a place But will Say come and try; you have a chance on condishions; that is But I will not wright to you the condistions; But will plainly tell you what they are when you come Mag if you want a home why you had better come soon as you get this dont tell any one but come right away when you come to the hospittle dont call for any one but me for i want to see you firt on perticular bisness\n",
      "\n",
      "yours\n",
      "\n",
      "E Stanton\n",
      "Va\n",
      "\n",
      "Dont you delay or you may be to late\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(\"https://valley.lib.virginia.edu/papers/A0010.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_valleydocs(ids, original=True, modern=False):\n",
    "    \"\"\"Scrape, clean, and download documents to .txt files\"\"\"\n",
    "    \n",
    "    valley_url = 'https://valley.lib.virginia.edu/' # Valley of the Shadow url\n",
    "    output = '/Users/nickclifford/Documents/UVA/Spring 2020/DS 5001 Exploratory Text Analysis/final/data/' # output file destination\n",
    "   \n",
    "    if original:\n",
    "        print(\"Bad Document IDs (orig):\")\n",
    "        # Downloads the original version\n",
    "        for doc in tqdm(ids, desc='Original IDs', unit='file'):\n",
    "            file = open(output + 'orig/' + doc.replace('.html', '.txt'), 'w')\n",
    "            try:\n",
    "                file.write(get_doc(valley_url + 'papers/'  + doc))\n",
    "                file.close()\n",
    "            # Error when Valley of the Shadow page is empty, not a letter, or doesnt state county of origin\n",
    "            except (AttributeError, IndexError, UnboundLocalError) as e :\n",
    "                file.close()\n",
    "                os.remove(file.name)\n",
    "                print(doc)\n",
    "\n",
    "    # Downloads the modern day spelling if specified\n",
    "    if modern:\n",
    "        print(\"\\nBad Document IDs (mod):\")\n",
    "        doc='0'\n",
    "        for doc in tqdm(ids, desc='Modern IDs', unit='file'):\n",
    "            file = open(output + 'mod/' + doc.replace('.html', '.txt'), 'w')\n",
    "            try:\n",
    "                file.write(get_doc(valley_url + 'mod/'  + doc.strip('.html')))\n",
    "                file.close()\n",
    "            # Error when Valley of the Shadow page is empty, not a letter, or doesnt state county of origin\n",
    "            except (AttributeError, IndexError, UnboundLocalError) as e: \n",
    "                file.close()\n",
    "                os.remove(file.name)\n",
    "                print(doc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Document IDs (orig):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a37c58c318047679c48fb47e6550a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Original IDs', max=1867.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0322.html\n",
      "A2000.html\n",
      "A3061.html\n",
      "F0060.html\n",
      "F0063.html\n",
      "F3505.html\n",
      "F3507.html\n",
      "F3509.html\n",
      "F3510.html\n",
      "F3511.html\n",
      "F3516.html\n",
      "F3518.html\n",
      "F3520.html\n",
      "F3521.html\n",
      "F3524.html\n",
      "F3525.html\n",
      "F3526.html\n",
      "F3527.html\n",
      "F3529.html\n",
      "F3531.html\n",
      "F3533.html\n",
      "F3534.html\n",
      "F3535.html\n",
      "F3536.html\n",
      "F3539.html\n",
      "F3540.html\n",
      "F6070.html\n",
      "\n",
      "\n",
      "Bad Document IDs (mod):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e8b0dfcb184324bd4bf4696fae7a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Modern IDs', max=1867.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3061.html\n",
      "F0060.html\n",
      "F0063.html\n",
      "F3505.html\n",
      "F3507.html\n",
      "F3509.html\n",
      "F3510.html\n",
      "F3511.html\n",
      "F3516.html\n",
      "F3518.html\n",
      "F3520.html\n",
      "F3521.html\n",
      "F3524.html\n",
      "F3525.html\n",
      "F3526.html\n",
      "F3527.html\n",
      "F3529.html\n",
      "F3531.html\n",
      "F3533.html\n",
      "F3534.html\n",
      "F3535.html\n",
      "F3536.html\n",
      "F3539.html\n",
      "F3540.html\n",
      "F6070.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_valleydocs(letter_ids, modern=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Valley of the Shadow: Two Communities in the American Civil War, University of Virginia Library (https://valley.lib.virginia.edu/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
